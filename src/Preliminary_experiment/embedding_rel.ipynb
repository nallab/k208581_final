{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipediaコーパスを利用した Word2vec による関係要素のembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, font_manager\n",
    "import seaborn as sns\n",
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロットする図のサイズ設定\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "# プロットする図のフォント設定\n",
    "font_manager.fontManager.addfont('/Library/Fonts/ipaexg.ttf')\n",
    "rc('font', family='IPAEXGothic')\n",
    "\n",
    "sns.set_context('talk')\n",
    "sns.set_style(\"ticks\") # スタイルをticksに\n",
    "sns.set(context='talk', style='ticks', font=[\"IPAEXGothic\"], font_scale=10/6, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/result/に格納されている全csvファイルの読み込み\n",
    "csv_files = glob.glob(os.path.join(\"./data/\", \"*.csv\"))\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    tmp_df = pd.read_csv(file)\n",
    "    df_list.append(tmp_df)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 「property要素 + not (wikiPage) + not (画像)」 の関係リンクを持つデータの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predの要素で, propertyが含まれる値を返す\n",
    "df_prep = df[df.pred.str.contains('property')]\n",
    "\n",
    "# 'wikiPage'を含まないprepertyを返す\n",
    "df_prep = df_prep[~df_prep['pred'].str.contains('wikiPage')]\n",
    "\n",
    "# '画像'を含まないprepertyを返す\n",
    "df_prep = df_prep[~df_prep['pred'].str.contains('画像')]\n",
    "\n",
    "df_prep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nan削除\n",
    "df_prep = df_prep.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- objデータが数値を持つデータを省く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# objデータに'数値'を含まない値を返す\n",
    "df_prep = df_prep[df_prep['obj'].apply(lambda x: pd.to_numeric(x, errors='coerce')).isnull()]\n",
    "        \n",
    "df_prep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2vec のmodel 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim.model の読み込み\n",
    "# 事前学習済みのモデルや提供されているモデルのパスを記入\n",
    "\n",
    "model = Word2Vec.load('../Models/japanese-word2vec-model-builder/word2vec.gensim.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを利用した文字のエンべディング\n",
    "## 未知語に関しては, 今回はNaNで対応\n",
    "def vectorize(model, word):\n",
    "    try:\n",
    "        output = model.wv[word]\n",
    "        return output\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj要素に関してはLOD同士が繋がっているため, 基本的にURIで記述されている.\n",
    "# また, (県の魚:〇〇)の要素や数値データが入っている場合もあるため, それぞれ最後尾の要素を値として扱う. \n",
    "def preprocessing(obj):\n",
    "    if type(obj) != str:\n",
    "        output = obj\n",
    "    else:\n",
    "        output = obj.split(\"/\")[-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec でのエンべディングを実施する\n",
    "    - key, obj から pred(rel) の分散表現ベクトルを求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## key のvectorization\n",
    "### key に関しては,preprocessingは特に必要ないと仮定\n",
    "\n",
    "df_prep['key_vec'] = df_prep['key'].map(lambda key:vectorize(model, key))\n",
    "is_key_notnull = df_prep['key_vec'].notnull()\n",
    "print(\"登録済みkeyの要素数は:\",len(df_prep[is_key_notnull]))\n",
    "print(\"登録済みkeyのユニーク要素の個数は:\", \\\n",
    "      df_prep[is_key_notnull]['key'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## obj のpreprocessing\n",
    "  ## DBpedia上で登録されているuri情報を取り除く\n",
    "\n",
    "uri = 'http://ja.dbpedia.org/resource/'\n",
    "obj_list = df_prep['obj'].map(lambda obj:obj.replace(uri,''))\n",
    "obj_list = obj_list.map(lambda obj:obj.replace('※',''))\n",
    "obj_list = obj_list.str.replace('[()]','', regex=True)\n",
    "obj_list = obj_list.str.replace('[（）]',' ', regex=True)\n",
    "obj_list = obj_list.str.replace('.+[「『]|[」』]','', regex=True)\n",
    "obj_list = obj_list.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_preprocessing_1(obj):\n",
    "    if type(obj) != str:\n",
    "        output = obj\n",
    "    elif len(obj.split(\"：\")) != 1:\n",
    "        output = obj.split(\"：\")[1]\n",
    "    elif len(obj.split(\":\")) != 1:\n",
    "        output = obj.split(\":\")[1]\n",
    "    else:\n",
    "        output = obj\n",
    "    return output.strip()  \n",
    "        \n",
    "def obj_preprocessing_2(obj):\n",
    "    if type(obj) != str:\n",
    "        output = obj\n",
    "    elif len(obj.split(\"_\")) != 1:\n",
    "        output = obj.split(\"_\")[0]\n",
    "    elif len(obj.split(\"、\")) != 1:\n",
    "        output = obj.split(\"、\")[0]\n",
    "    elif len(obj.split(\" \")) != 1:\n",
    "        output = obj.split(\" \")[0]\n",
    "    elif len(obj.split(\"、\")) != 1:\n",
    "        output = obj.split(\"、\")[0]\n",
    "    elif len(obj.split(\"・\")) != 1:\n",
    "        output = obj.split(\"・\")[0]\n",
    "    elif len(obj.split(\"・\")) != 1:\n",
    "        output = obj.split(\"・\")[0]\n",
    "    else:\n",
    "        output = obj\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = obj_list.map(lambda obj:obj_preprocessing_1(obj))\n",
    "obj_list = obj_list.map(lambda obj:obj_preprocessing_2(obj))\n",
    "df_prep['obj_vec'] = obj_list.map(lambda obj:vectorize(model, obj))\n",
    "df_prep['pre_obj'] = obj_list\n",
    "is_obj_notnull = df_prep['obj_vec'].notnull()\n",
    "print(\"登録済みobjの要素数は:\",len(df_prep[is_obj_notnull]))\n",
    "print(\"登録済みobjのユニーク要素の個数は:\", \\\n",
    "      df_prep[is_obj_notnull]['obj'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- key + rel = obj が成り立つと仮定する.\n",
    "  - そこで, rel は, obj - key で求められるとする.\n",
    "  - その後, 求めたrel を要素ごとの値ではなく関係ごとの値として定める."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = df_prep.loc[is_key_notnull & is_obj_notnull]\n",
    "df_prep['pred_vec'] = df_prep['obj_vec'] - df_prep['key_vec']\n",
    "print(\"pred_vec がnullの要素数は：\",df_prep['pred_vec'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep['label'] = df_prep['key'] + '/' + df_prep['pre_obj']\n",
    "df = df_prep.loc[:,['key','pred','pre_obj','label','key_vec','obj_vec','pred_vec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df,'../../pickles/pre_exp_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f62dfc091dd6cfa1d3865ae78a24044e34e7cf2ceaaa30517d6a6b400f5d2b77"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "db33b9f7f7cfa92ad9f742d45b74c770b03f299969fa4d08d31fb7fd7d681032"
   }
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
