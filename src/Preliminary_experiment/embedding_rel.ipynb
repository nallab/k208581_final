{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipediaコーパスを利用した Word2vec による関係要素のembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, font_manager\n",
    "import seaborn as sns\n",
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロットする図のサイズ設定\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.rcParams[\"font.size\"] = 13\n",
    "# プロットする図のフォント設定\n",
    "font_manager.fontManager.addfont('/Library/Fonts/ipaexg.ttf')\n",
    "rc('font', family='IPAEXGothic')\n",
    "\n",
    "sns.set_context('talk')\n",
    "sns.set_style(\"ticks\") # スタイルをticksに\n",
    "sns.set(context='talk', style='ticks', font=[\"IPAEXGothic\"], font_scale=10/6, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data/result/に格納されている全csvファイルの読み込み\n",
    "csv_files = glob.glob(os.path.join(\"./data/\", \"*.csv\"))\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    tmp_df = pd.read_csv(file)\n",
    "    df_list.append(tmp_df)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 「property要素 + not (wikiPage) + not (画像)」 の関係リンクを持つデータの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predの要素で, propertyが含まれる値を返す\n",
    "df_prep = df[df.pred.str.contains('property')]\n",
    "\n",
    "# 'wikiPage'を含まないprepertyを返す\n",
    "df_prep = df_prep[~df_prep['pred'].str.contains('wikiPage')]\n",
    "\n",
    "# '画像'を含まないprepertyを返す\n",
    "df_prep = df_prep[~df_prep['pred'].str.contains('画像')]\n",
    "\n",
    "df_prep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nan削除\n",
    "df_prep = df_prep.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- objデータが数値を持つデータを省く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# objデータに'数値'を含まない値を返す\n",
    "df_prep = df_prep[df_prep['obj'].apply(lambda x: pd.to_numeric(x, errors='coerce')).isnull()]\n",
    "        \n",
    "df_prep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2vec のmodel 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim.model の読み込み\n",
    "# 事前学習済みのモデルや提供されているモデルのパスを記入\n",
    "\n",
    "# model = Word2Vec.load('../../Models/japanese-word2vec-model-builder/word2vec.gensim.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを利用した文字のエンべディング\n",
    "## 未知語に関しては, 今回はNaNで対応\n",
    "def vectorize(model, word):\n",
    "    try:\n",
    "        output = model.wv[word]\n",
    "        return output\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj要素に関してはLOD同士が繋がっているため, 基本的にURIで記述されている.\n",
    "# また, (県の魚:〇〇)の要素や数値データが入っている場合もあるため, それぞれ最後尾の要素を値として扱う. \n",
    "def preprocessing(obj):\n",
    "    if type(obj) != str:\n",
    "        output = obj\n",
    "    else:\n",
    "        output = obj.split(\"/\")[-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec でのエンべディングを実施する\n",
    "    - key, obj から pred(rel) の分散表現ベクトルを求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手法\n",
    "## key は, 基本的flagsの形式である. exe: 沖縄県, ボブサップ, さつまいも\n",
    "## obj は, URI形式と(数)単語を含む自然文の形式である. exe: http://ja.dbpedia.org/resource/〇〇, 那覇市\n",
    "    \n",
    "cp_df = df_prep.copy()\n",
    "cp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## key のvectorization\n",
    "### key に関しては,preprocessingは特に必要ないと仮定\n",
    "\n",
    "cp_df['key_vec'] = cp_df['key'].map(lambda key:vectorize(model, key))\n",
    "is_key_notnull = cp_df['key_vec'].notnull()\n",
    "print(\"登録済みkeyの要素数は:\",len(cp_df[is_key_notnull]))\n",
    "print(\"登録済みkeyのユニーク要素の個数は:\", \\\n",
    "      cp_df[is_key_notnull]['key'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## obj のpreprocessing\n",
    "  ## DBpedia上で登録されているuri情報を取り除く\n",
    "\n",
    "uri = 'http://ja.dbpedia.org/resource/'\n",
    "obj_list = cp_df['obj'].map(lambda obj:obj.replace(uri,''))\n",
    "obj_list = obj_list.map(lambda obj:obj.replace('※',''))\n",
    "obj_list = obj_list.str.replace('[()]','', regex=True)\n",
    "obj_list = obj_list.str.replace('[（）]',' ', regex=True)\n",
    "obj_list = obj_list.str.replace('.+[「『]|[」』]','', regex=True)\n",
    "obj_list = obj_list.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_preprocessing_1(obj):\n",
    "    if type(obj) != str:\n",
    "        output = obj\n",
    "    elif len(obj.split(\"：\")) != 1:\n",
    "        output = obj.split(\"：\")[1]\n",
    "    elif len(obj.split(\":\")) != 1:\n",
    "        output = obj.split(\":\")[1]\n",
    "    else:\n",
    "        output = obj\n",
    "    return output.strip()  \n",
    "        \n",
    "def obj_preprocessing_2(obj):\n",
    "    if type(obj) != str:\n",
    "        output = obj\n",
    "    elif len(obj.split(\"_\")) != 1:\n",
    "        output = obj.split(\"_\")[0]\n",
    "    elif len(obj.split(\"、\")) != 1:\n",
    "        output = obj.split(\"、\")[0]\n",
    "    elif len(obj.split(\" \")) != 1:\n",
    "        output = obj.split(\" \")[0]\n",
    "    elif len(obj.split(\"、\")) != 1:\n",
    "        output = obj.split(\"、\")[0]\n",
    "    elif len(obj.split(\"・\")) != 1:\n",
    "        output = obj.split(\"・\")[0]\n",
    "    elif len(obj.split(\"・\")) != 1:\n",
    "        output = obj.split(\"・\")[0]\n",
    "    else:\n",
    "        output = obj\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = obj_list.map(lambda obj:obj_preprocessing_1(obj))\n",
    "obj_list = obj_list.map(lambda obj:obj_preprocessing_2(obj))\n",
    "cp_df['obj_vec'] = obj_list.map(lambda obj:vectorize(model, obj))\n",
    "cp_df['pre_obj'] = obj_list\n",
    "is_obj_notnull = cp_df['obj_vec'].notnull()\n",
    "print(\"登録済みobjの要素数は:\",len(cp_df[is_obj_notnull]))\n",
    "print(\"登録済みobjのユニーク要素の個数は:\", \\\n",
    "      cp_df[is_obj_notnull]['obj'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- key + rel = obj が成り立つと仮定する.\n",
    "  - そこで, rel は, obj - key で求められるとする.\n",
    "  - その後, 求めたrel を要素ごとの値ではなく関係ごとの値として定める."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df = cp_df.loc[is_key_notnull & is_obj_notnull]\n",
    "\n",
    "cp_df['pred_vec'] = cp_df['obj_vec'] - cp_df['key_vec']\n",
    "# display(cp_df[is_obj_notnull].head(3))\n",
    "\n",
    "print(\"pred_vec がnullの要素数は：\",cp_df['pred_vec'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 合成ベクトル or 重心ベクトルを求めてみる.\n",
    "sum_vec = cp_df.groupby('pred')['pred_vec'].apply(lambda x:np.sum(x))\n",
    "cent_vec = cp_df.groupby('pred')['pred_vec'].apply(lambda x:np.sum(x)/x.count())\n",
    "cp_pred_df = pd.DataFrame([sum_vec,cent_vec]).T\n",
    "\n",
    "cp_pred_df.columns = ['sum_vec','cent_vec']\n",
    "# display(cp_pred_df.sample(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wikipediaをコーパスとする分散表現の獲得が達成\n",
    "    - cp1_pred_df ← pred(rel)が持つURI形式を外し,単語にしたものをembedding\n",
    "    - cp_pred_df ← key, obj をembeddingし, 仮定式： key - rel = obj を置いたときの合成ベクトルと重心ベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 階層クラスタリングを用いて曖昧さを考慮したまとまりの作成を実施する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster, cophenet, dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 要素毎(predでまとめない)の階層クラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp_df.duplicated(subset=['key','obj']).sum()\n",
    "# 179\n",
    "\n",
    "is_key_obj_duplicated = cp_df.duplicated(subset=['key','pre_obj'])\n",
    "cp_element_df = cp_df.loc[~is_key_obj_duplicated,['key','pred','pre_obj','pred_vec']].reset_index(drop=True)\n",
    "cp_element_df['label'] = cp_element_df['key'] + '/' + cp_element_df['pre_obj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_element_df.loc[:,['key','pred','pre_obj']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_element_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 知っていたが, 流石に1008は多すぎるな."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pred_vec = cp_element_df['pred_vec'].tolist()\n",
    "pred_vec_idnex = cp_element_df.index\n",
    "\n",
    "result = linkage(list_pred_vec,\n",
    "                 method='ward',\n",
    "                 metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1 * np.max(result[:, 2])\n",
    "threshold2 = 0.15 * np.max(result[:, 2])\n",
    "threshold3 = 0.2 * np.max(result[:, 2])\n",
    "threshold4 = 0.25 * np.max(result[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,10))\n",
    "\n",
    "dendrogram(result,\n",
    "           labels=list(cp_element_df['label']),\n",
    "           color_threshold=threshold3)\n",
    "\n",
    "ax.axhline(threshold3, linestyle='--', color='r')\n",
    "sns.despine()\n",
    "# plt.title(\"pred_cluster_02\")\n",
    "# ax.legend()\n",
    "ax.set(xlabel = '', ylabel='Threshold')\n",
    "plt.xticks(fontsize=13)\n",
    "\n",
    "ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True)) \n",
    "# plt.savefig('./output/02_cluster.png', transparent = True, bbox_inches='tight')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion は, fcluster 作成でのクラスタ選びのアルゴリズム\n",
    "# 他にもいろんなアルゴリズムがある\n",
    "# fcluster の出力_array のindex は, 入力データのindex に属する \n",
    "\n",
    "cluster1 = fcluster(result,\n",
    "                    threshold,\n",
    "                    criterion='distance')\n",
    "\n",
    "cluster2 = fcluster(result,\n",
    "                    threshold2,\n",
    "                    criterion='distance')\n",
    "\n",
    "cluster3 = fcluster(result,\n",
    "                    threshold3,\n",
    "                    criterion='distance')\n",
    "\n",
    "cluster4 = fcluster(result,\n",
    "                    threshold4,\n",
    "                    criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 階層クラスター分析の結果をDataFrame化\n",
    "_cluster = pd.DataFrame({'class_thre_0.1':cluster1,\n",
    "                         'class_thre_0.15':cluster2,\n",
    "                         'class_thre_0.2':cluster3,\n",
    "                         'class_thre_0.25':cluster4,\n",
    "                        }\n",
    "                        , index = pred_vec_idnex)\n",
    "\n",
    "display(_cluster.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元データと分析結果を結合\n",
    "cluster_df = pd.concat([cp_element_df, _cluster] ,axis=1)\n",
    "display(cluster_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(cluster_df, '../../pickles/cluster_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f62dfc091dd6cfa1d3865ae78a24044e34e7cf2ceaaa30517d6a6b400f5d2b77"
  },
  "kernelspec": {
   "display_name": "develop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "db33b9f7f7cfa92ad9f742d45b74c770b03f299969fa4d08d31fb7fd7d681032"
   }
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
